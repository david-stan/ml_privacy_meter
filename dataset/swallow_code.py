from functools import partial
from typing import Callable, Optional, Dict, Any

from datasets import load_dataset, Dataset
from transformers import PreTrainedTokenizer


def replace_label_with_input_ids(
    example: Dict[str, Any], tokenizer: PreTrainedTokenizer
) -> Dict[str, Any]:
    """
    Replace the label column with the input_ids generated by the tokenizer.

    Args:
        example (Dict[str, Any]): A single example from the dataset.
        tokenizer (PreTrainedTokenizer): The tokenizer to use for tokenization.

    Returns:
        Dict[str, Any]: The modified example with input_ids and labels.
    """
    tokenized_inputs = tokenizer(
        example["text"], padding="max_length", truncation=True, max_length=512
    )
    tokenized_inputs["labels"] = tokenized_inputs[
        "input_ids"
    ].copy()  # Duplicate input_ids to labels

    return tokenized_inputs


def load_swallow_code(
    preprocessing_fn: Optional[
        Callable[[Dict[str, Any]], Dict[str, Any]]
    ] = None,
    tokenize: bool = False,
    tokenizer: Optional[PreTrainedTokenizer] = None,
) -> Dataset:
    """
    Load the Swallow Code dataset from Hugging Face datasets library.
    If a preprocessing function is provided, apply it to the dataset.
    If tokenize is True, tokenize the dataset using the provided tokenizer.
    The input_ids will be used to replace the original "label" column.

    Args:
        split (str): The split of the dataset to load.
        preprocessing_fn (Optional[Callable[[Dict[str, Any]], Dict[str, Any]]]): A function to preprocess the dataset.
        tokenize (bool): Whether to tokenize the dataset.
        tokenizer (Optional[PreTrainedTokenizer]): The tokenizer to use for tokenization.

    Returns:
        Dataset: The AG News dataset.
    """
    swallow_code = load_dataset("tokyotech-llm/swallow-code", "swallow-code", streaming=True, split="train")
    if preprocessing_fn is not None:
        swallow_code = swallow_code.map(preprocessing_fn)
        print("Preprocessing function applied to the dataset.")

    if tokenize:
        if tokenizer is None:
            raise ValueError("Tokenizer is not provided. Cannot tokenize the dataset.")

        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            print(
                "The tokenizer pad token is None. Setting it to the EOS token for padding. "
                "If this is not desired, please set the pad token manually and "
                "save the tokenizer to disk before loading again."
            )

        swallow_code = swallow_code.map(
            partial(replace_label_with_input_ids, tokenizer=tokenizer),
            batched=True,
            remove_columns=["label"],
        )

    return swallow_code
