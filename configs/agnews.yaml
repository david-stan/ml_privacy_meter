run: # Configurations for a specific run
  random_seed: 12345 # Integer number of specifying random seed
  log_dir: demo_agnews # String for indicating where to save all the information, including models and computed signals. We can reuse the models saved in the same log_dir.
  time_log: True # Indicate whether to log the time for each step

audit: # Configurations for auditing
  algorithm: RMIA # String for indicating the membership inference attack. We currently support the RMIA introduced by Zarifzadeh et al. 2024(https://openreview.net/pdf?id=sT7UJh5CTc)) and the LOSS attacks
  num_ref_models: 1 # Number of reference models used to audit each target model
  device: cuda:0 # String for indicating the device we want to use for inferring signals and auditing models
  batch_size: 10 # Integer number for indicating batch size for evaluating models and inferring signals.
  data_size: 1000
  population_size: 100

train: # Configuration for training
  model_name: gpt2 # String for indicating the model type. We support CNN, wrn28-1, wrn28-2, wrn28-10, vgg16, mlp, gpt2 and speedyresnet (requires cuda). More model types can be added in model.py.
  device: cuda:0 # String for indicating the device we want to use for training models.
  batch_size: 8
  auto_find_batch_size: True # When set to True, the batch size will be automatically set by the trainer according to the hardware's capability. Recommended for less powerful GPUs
  learning_rate: 0.000001
  weight_decay: 0.01
  epochs: 3
  optimizer: adamw_torch
  save_checkpoints: True
  restart_from_checkpoint: True
  eval_steps: 50
  num_samples: 30000

data: # Configuration for data
  dataset: agnews # String indicates the name of the dataset. We support cifar10, cifar100, purchase100 and texas100 and agnews by default.
  data_dir: data
  tokenize: True
  tokenizer: gpt2 # String for indicating the tokenizer type. It can be any tokenizer or local checkpoint supported by the transformers library.