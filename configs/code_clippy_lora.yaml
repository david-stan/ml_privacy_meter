run: # Configurations for a specific run
  random_seed: 56 # Integer number of specifying random seed
  log_dir: demo_swallow_code_lora # String for indicating where to save all the information, including models and computed signals. We can reuse the models saved in the same log_dir.
  time_log: True # Indicate whether to log the time for each step
  num_experiments: 1 # Number of total experiments

audit: # Configurations for auditing
  privacy_game: privacy_loss_model # Indicate the privacy game from privacy_loss_model
  algorithm: RMIA # String for indicating the membership inference attack. We currently support the RMIA introduced by Zarifzadeh et al. 2024(https://openreview.net/pdf?id=sT7UJh5CTc)) and the LOSS attacks
  num_ref_models: 1 # Number of reference models used to audit each target model
  device: cuda:0 # String for indicating the device we want to use for inferring signals and auditing models
  report_log: report_rmia # String that indicates the folder where we save the log and auditing report
  batch_size: 10 # Integer number for indicating batch size for evaluating models and inferring signals.
  data_size: 1000
  population_size: 100

train: # Configuration for training
  model_name: shibing624/code-autocomplete-gpt2-base # String for indicating the model type. We support CNN, wrn28-1, wrn28-2, wrn28-10, vgg16, mlp, gpt2 and speedyresnet (requires cuda). More model types can be added in model.py.
  tokenizer: shibing624/code-autocomplete-gpt2-base # String for indicating the tokenizer type. It can be any tokenizer or local checkpoint supported by the transformers library.
  device: cuda:0 # String for indicating the device we want to use for training models.
  batch_size: 8
  auto_find_batch_size: False # When set to True, the batch size will be automatically set by the trainer according to the hardware's capability. Recommended for less powerful GPUs
  learning_rate: 0.000001
  weight_decay: 0.01
  epochs: 3
  optimizer: adamw_torch
  save_checkpoints: True
  eval_steps: 100
  num_samples: 50000
  peft:
    type: lora # Only lora is supported
    fan_in_fan_out: True
    r: 8
    target_modules: ["c_attn", "c_proj", "c_fc"]

data: # Configuration for data
  dataset: code_clippy # String indicates the name of the dataset. We support cifar10, cifar100, purchase100 and texas100 and agnews by default.
  data_dir: data
  tokenize: True
  tokenizer: shibing624/code-autocomplete-gpt2-base # String for indicating the tokenizer type. It can be any tokenizer or local checkpoint supported by the transformers library.